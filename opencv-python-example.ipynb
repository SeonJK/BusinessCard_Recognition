{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]\n",
      "opencv: 3.4.5\n",
      "pytesseract: pytesseract version 0.1.6\n"
     ]
    }
   ],
   "source": [
    "# OpenCV 및 OCR모듈 설치 유무 확인\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# 윈도우에서 주석해제 (tesseract_path: tesseract설치경로 확인 후 붙여넣기)\n",
    "tesseract_path = 'C:/Program Files (x86)/Tesseract-OCR'\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_path + '/tesseract'\n",
    "\n",
    "print (\"python:\", sys.version)\n",
    "print (\"opencv:\", cv2.__version__)\n",
    "print (\"pytesseract:\", pytesseract.image_to_string(Image.open('images/test.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 이미지 읽기, 쓰기 및 표시하기 (1)\n",
    "\n",
    "import cv2\n",
    "\n",
    "def handle_image():\n",
    "    imgfile = 'images/sample.png'\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_COLOR) # imread(파일경로, 이미지읽는 방식)\n",
    "    \n",
    "    cv2.imshow('image', img) # imshow(윈도우 타입, 이미지 객체)\n",
    "    \n",
    "    cv2.waitKey(0) # 키보드입력을 기다림(0=무한대)\n",
    "    cv2.destroyAllWindows() # 모든 윈도우 닫음\n",
    "    #cv2.waitKey(1) # 주피터 버그로 인해 작성\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    handle_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 이미지 읽기, 쓰기 및 표시하기 (2)\n",
    "\n",
    "import cv2\n",
    "\n",
    "def handle_image():\n",
    "    imgfile = 'images/sample.png'\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_GRAYSCALE) # IMREAD_GRAYSCALE - 흑백 이미지\n",
    "    \n",
    "    cv2.namedWindow('image', cv2.WINDOW_NORMAL) \n",
    "    # 윈도우창의 속성지정(윈도우타입, 원본이미지크기/조절가능)\n",
    "    cv2.imshow('image', img)\n",
    "    k = cv2.waitKey(0) \n",
    "    # wait for ESC key to exit\n",
    "    if k == 27:\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.waitKey(1)\n",
    "    # wait for 's' key to save and exit\n",
    "    elif k == ord('s'):\n",
    "        cv2.imwrite('grayImage.png', img) # imwrite(저장하려는 이름, 이미지 객체)\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    handle_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 도형 외곽 추출하기 (1)\n",
    "\n",
    "import cv2\n",
    "\n",
    "def contour():\n",
    "    imgfile = 'images/contour.jpg'\n",
    "    img = cv2.imread(imgfile)\n",
    "    imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # 색공간을 바꾸는 파라미터{RGB->회색}\n",
    "    # 두개의 이미지를 저장하는 이유 - 나중에 외곽추출시 원본 이미지를 가져와서 변형하기때문\n",
    "    \n",
    "    edge = cv2.Canny(imgray, 100, 200)\n",
    "    # 외곽찾기 Canny알고리즘(이미지객체, threshold값2개(100미만은 외곽X, 200초과만 추출))\n",
    "    # 엣지간의 연결성 강조 => maxVal와 minVal 사이에 있더라도 연결성을 고려하여 엣지로 추출\n",
    "    edge, contours, hierarchy = cv2.findContours(edge, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #외곽을 찾음 findContours(엣지, 트리관계로 배열반환, 꼭지점or모든 좌표 반환 설정파라미터)\n",
    "    cv2.imshow('edge', edge)\n",
    "    cv2.drawContours(img, contours, -1, (0, 255, 0), 1)\n",
    "    # 외곽그리는 함수(이미지객체, 외곽을 찾은 객체, contour의 index{-1=전체}, BGR값, 선 두께)\n",
    "    cv2.imshow('Contour', img)\n",
    "    cv2.imshow('imgGray', imgray)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    contour() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 도형 외곽 추출하기 (2)\n",
    "\n",
    "import cv2\n",
    "\n",
    "def contour_approx():\n",
    "    imgfile = 'images/contour2.png'\n",
    "    img = cv2.imread(imgfile)\n",
    "    img2 = img.copy()\n",
    "    imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    edge = cv2.Canny(imgray, 100, 200)\n",
    "    edge, contours, hierarchy = cv2.findContours(edge, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    cnt = contours[0]\n",
    "    cv2.drawContours(img, [cnt], 0, (0, 255, 0), 3)\n",
    "    \n",
    "    epsilon = 0.1 * cv2.arcLength(cnt, True)\n",
    "    #epsilon: 근사 정확도, contour의 둘레길이 계산(contour, True=페곡선)\n",
    "    #보통 2%-5%로 오차를 설정\n",
    "    approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "    #다각형의 꼭짓점을 줄여나가는 함수\n",
    "    # approxPolyDP(꼭짓점을 줄일 contour , epsilon이 작을수록 원본과 흡사, True=폐곡선) \n",
    "    cv2.drawContours(img2, [approx], 0, (0, 255, 0), 3)\n",
    "    \n",
    "    cv2.imshow('Contour', img)\n",
    "    cv2.imshow('Approx', img2)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    contour_approx() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 투영변환 구현하기 (1)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def warp_affine():\n",
    "    img = cv2.imread('images/transform.png')\n",
    "    \n",
    "    pts1 = np.float32([[50, 50], [200, 50], [20, 200]])\n",
    "    pts2 = np.float32([[70, 100], [220, 50], [150, 250]])\n",
    "    \n",
    "    M = cv2.getAffineTransform(pts1, pts2) # (이동당할 배열, 이동할 배열)\n",
    "    \n",
    "    result = cv2.warpAffine(img, M, (350, 300))\n",
    "    #몇가지 좌표를 이동시키면 다른 좌표도 모두 바뀐다\n",
    "    cv2.imshow('original', img)\n",
    "    cv2.imshow('Affine Transform', result)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    warp_affine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 투영변환 구현하기 (2)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def warp_perspective():\n",
    "    img = cv2.imread('images/transform.jpg')\n",
    "    \n",
    "    topLeft = [127, 157]\n",
    "    topRight = [448, 152]\n",
    "    bottomRight = [579, 526]\n",
    "    bottomLeft = [54, 549]\n",
    "    \n",
    "    pts1 = np.float32([topLeft, topRight, bottomRight, bottomLeft])\n",
    "    \n",
    "    w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "    w2 = abs(topRight[0] - topLeft[0])\n",
    "    h1 = abs(topRight[1] - bottomRight[1])\n",
    "    h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "    minWidth = min([w1, w2])\n",
    "    minHeight = min([h1, h2])\n",
    "    \n",
    "    pts2 = np.float32([[0,0], [minWidth-1,0], \n",
    "                      [minWidth-1,minHeight-1], [0,minHeight-1]])\n",
    "    # 1을 뺀 이유 - 외곽에 흰색으로 픽셀이 남기 때문\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "    #getAffingTransform과 getPerspectiveTrasform의 차이 - Perspective는 원근보정 지원\n",
    "    result = cv2.warpPerspective(img, M, (int(minWidth), int(minHeight)))\n",
    "    #(이미지객체, 픽셀이동배열, 변환될 이미지의 크기)\n",
    "    cv2.imshow('original', img)\n",
    "    cv2.imshow('Warp Transform', result)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    warp_perspective()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 스캔한 듯한 효과 주기 (1)\n",
    "# 흰색, 검은색만 사용하도록 바꿔주기\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Callback Function for Trackbar (but do not any work)\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "def global_threshold():\n",
    "    imgfile = 'images/document.jpg'\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize image\n",
    "    r = 600.0 / img.shape[0] # 600/가로길이\n",
    "    dim = (int(img.shape[1] * r), 600) # 세로길이*가로길이, 600\n",
    "    img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    WindowName = \"Window\"\n",
    "    TrackbarName = \"Threshold\"\n",
    "    \n",
    "    # Make Window and Trackbar\n",
    "    cv2.namedWindow(WindowName)\n",
    "    cv2.createTrackbar(TrackbarName, WindowName, 70, 255, nothing)\n",
    "    #Threshold를 위한 트랙바를 만듬(이름, 윈도우이름, Threshold초기값, Threshold최대값, 콜백함수))\n",
    "    \n",
    "    # Allocate destination image\n",
    "    Threshold = np.zeros(img.shape, np.uint8)\n",
    "    # zeros(이미지가로세로길이반환) - 모든것을 0으로 초기화\n",
    "    \n",
    "    # Loop for get trackbar pos and process it\n",
    "    while True:\n",
    "        # Get position in trackbar\n",
    "        TrackbarPos = cv2.getTrackbarPos(TrackbarName, WindowName)\n",
    "        # 트랙바위치반환\n",
    "        \n",
    "        # Apply threshold\n",
    "        cv2.threshold(img, TrackbarPos, 255, cv2.THRESH_BINARY, Threshold)\n",
    "        # (이미지, trheshold값, 최댓값, 함수적용할 알고리즘, 이진화된 결과물)\n",
    "        \n",
    "        # Show in window\n",
    "        cv2.imshow(WindowName, Threshold)\n",
    "        \n",
    "        # wait for ESC key to exit\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            cv2.waitKey(1)\n",
    "            break\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    global_threshold() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV - 스캔한 듯한 효과 주기 (2)\n",
    "# Adaptive Threshold: 조명에 상관없이 세밀한 영역으로 나누어 주변영역의 밝기 평균에\n",
    "# 일정한 상수를 더하거나 빼서 Threshold를 결정한다.\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def adaptive_threshold():\n",
    "    imgfile = 'images/document.jpg'\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize image\n",
    "    r = 600.0 / img.shape[0]\n",
    "    dim = (int(img.shape[1] * r), 600)\n",
    "    img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    # Blur image and apply adaptive threshold\n",
    "    blur = cv2.GaussianBlur(img, (9, 9), 0)\n",
    "    # GaussianBlur(이미지객체, 주변픽셀크기, )\n",
    "    # Blur-이미지를 흐리게 함(주변 pixel의 평균값을 대입) / 노이즈를 줄이게 하기 위함\n",
    "    # Blur효과를 사용할 경우 텍스트는 뭉개지기 때문에 엣지검출에만 사용하고\n",
    "    # Blur이미지는 버린다.\n",
    "    result_without_blur = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "    # 21 - 쪼개는 정도, 10 - 빼는 상수값 ( 일반적으로 추천되는 값들이다. )\n",
    "    result_with_blur = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "    cv2.imshow('Without Blur', result_without_blur)\n",
    "    cv2.imshow('With Blur', result_with_blur)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    adaptive_threshold() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Edge Detection\n",
      "STEP 2: Find contours of paper\n",
      "STEP 3: Apply perspective transform\n",
      "STEP 4: Apply Adaptive Threshold\n"
     ]
    }
   ],
   "source": [
    "# 명함인식 구현하기 - 캡처된 이미지\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def order_points(pts):\n",
    "    # initialzie a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    # axis = 1 => 각 행끼리 비교 (x+y)\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    # (y-x)\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "def auto_scan_image():\n",
    "    # load the image and compute the ratio of the old height\n",
    "    # to the new height, clone it, and resize it\n",
    "    # document.jpg ~ docuemnt7.jpg\n",
    "    image = cv2.imread('images/document7.jpg')\n",
    "    orig = image.copy()\n",
    "    r = 800.0 / image.shape[0]\n",
    "    dim = (int(image.shape[1] * r), 800)\n",
    "    image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    # convert the image to grayscale, blur it, and find edges\n",
    "    # in the image\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # 이미지 색 변경\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    # Blur효과\n",
    "    edged = cv2.Canny(gray, 70, 200)\n",
    "    # Edge 검출\n",
    "\n",
    "    # show the original image and the edge detected image\n",
    "    print (\"STEP 1: Edge Detection\")\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.imshow(\"Edged\", edged)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    # find the contours in the edged image, keeping only the\n",
    "    # largest ones, and initialize the screen contour\n",
    "    (_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # 계층관계는 필요없기때문에 cnts만 명시\n",
    "    cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "    # 컨투어들 중 큰 값인 5개까지 받아온다.\n",
    "\n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "        # approximate the contour\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        # 컨투어 길이 반환\n",
    "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "        # 2% 근사해서 외곽을 추출\n",
    "\n",
    "        # if our approximated contour has four points, then we\n",
    "        # can assume that we have found our screen\n",
    "        if len(approx) == 4:\n",
    "            screenCnt = approx\n",
    "            break\n",
    "\n",
    "    # show the contour (outline) of the piece of paper\n",
    "    print (\"STEP 2: Find contours of paper\")\n",
    "    cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Outline\", image)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    # apply the four point transform to obtain a top-down\n",
    "    # view of the original image\n",
    "    rect = order_points(screenCnt.reshape(4, 2) / r)\n",
    "    (topLeft, topRight, bottomRight, bottomLeft) = rect\n",
    "    \n",
    "    w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "    w2 = abs(topRight[0] - topLeft[0])\n",
    "    h1 = abs(topRight[1] - bottomRight[1])\n",
    "    h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "    maxWidth = max([w1, w2])\n",
    "    maxHeight = max([h1, h2])\n",
    "    \n",
    "    dst = np.float32([[0,0], [maxWidth-1,0], \n",
    "                      [maxWidth-1,maxHeight-1], [0,maxHeight-1]])\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(orig, M, (maxWidth, maxHeight))\n",
    "\n",
    "    # show the original and scanned images\n",
    "    print (\"STEP 3: Apply perspective transform\")\n",
    "    cv2.imshow(\"Warped\", warped)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "    # convert the warped image to grayscale, then threshold it\n",
    "    # to give it that 'black and white' paper effect\n",
    "    warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "    warped = cv2.adaptiveThreshold(warped, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "\n",
    "    # show the original and scanned images\n",
    "    print (\"STEP 4: Apply Adaptive Threshold\")\n",
    "    cv2.imshow(\"Original\", orig)\n",
    "    cv2.imshow(\"Scanned\", warped)\n",
    "    cv2.imwrite('scannedImage.png', warped)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "465.0\n",
      "307200\n",
      "0.001513671875\n",
      "STEP 1: Edge Detection\n",
      "465.0\n",
      "307200\n",
      "0.001513671875\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "487.0\n",
      "307200\n",
      "0.0015852864583333333\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "465.0\n",
      "307200\n",
      "0.001513671875\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "495.0\n",
      "307200\n",
      "0.001611328125\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "74.0\n",
      "307200\n",
      "0.00024088541666666667\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "495.0\n",
      "307200\n",
      "0.001611328125\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "495.0\n",
      "307200\n",
      "0.001611328125\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "465.0\n",
      "307200\n",
      "0.001513671875\n",
      "STEP 1: Edge Detection\n",
      "439.5\n",
      "307200\n",
      "0.0014306640625\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "495.0\n",
      "307200\n",
      "0.001611328125\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "502.0\n",
      "307200\n",
      "0.0016341145833333333\n",
      "STEP 1: Edge Detection\n",
      "502.0\n",
      "307200\n",
      "0.0016341145833333333\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "487.0\n",
      "307200\n",
      "0.0015852864583333333\n",
      "STEP 1: Edge Detection\n",
      "443.0\n",
      "307200\n",
      "0.0014420572916666666\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "443.0\n",
      "307200\n",
      "0.0014420572916666666\n",
      "STEP 1: Edge Detection\n",
      "487.0\n",
      "307200\n",
      "0.0015852864583333333\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "55.0\n",
      "307200\n",
      "0.00017903645833333332\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "55.0\n",
      "307200\n",
      "0.00017903645833333332\n",
      "STEP 1: Edge Detection\n",
      "495.0\n",
      "307200\n",
      "0.001611328125\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "465.0\n",
      "307200\n",
      "0.001513671875\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "443.0\n",
      "307200\n",
      "0.0014420572916666666\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "55.0\n",
      "307200\n",
      "0.00017903645833333332\n",
      "STEP 1: Edge Detection\n",
      "487.0\n",
      "307200\n",
      "0.0015852864583333333\n",
      "STEP 1: Edge Detection\n",
      "472.5\n",
      "307200\n",
      "0.0015380859375\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "487.0\n",
      "307200\n",
      "0.0015852864583333333\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "55.0\n",
      "307200\n",
      "0.00017903645833333332\n",
      "STEP 1: Edge Detection\n",
      "61.0\n",
      "307200\n",
      "0.00019856770833333334\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "443.0\n",
      "307200\n",
      "0.0014420572916666666\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "465.0\n",
      "307200\n",
      "0.001513671875\n",
      "STEP 1: Edge Detection\n",
      "472.0\n",
      "307200\n",
      "0.0015364583333333333\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "58.5\n",
      "307200\n",
      "0.0001904296875\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "509.0\n",
      "307200\n",
      "0.0016569010416666668\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "443.0\n",
      "307200\n",
      "0.0014420572916666666\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "479.0\n",
      "307200\n",
      "0.0015592447916666667\n",
      "STEP 1: Edge Detection\n",
      "465.0\n",
      "307200\n",
      "0.001513671875\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "472.0\n",
      "307200\n",
      "0.0015364583333333333\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "502.0\n",
      "307200\n",
      "0.0016341145833333333\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "469.0\n",
      "307200\n",
      "0.0015266927083333332\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "443.0\n",
      "307200\n",
      "0.0014420572916666666\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "494.0\n",
      "307200\n",
      "0.0016080729166666667\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "439.5\n",
      "307200\n",
      "0.0014306640625\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "22.5\n",
      "307200\n",
      "7.32421875e-05\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "494.0\n",
      "307200\n",
      "0.0016080729166666667\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "502.0\n",
      "307200\n",
      "0.0016341145833333333\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "494.0\n",
      "307200\n",
      "0.0016080729166666667\n",
      "STEP 1: Edge Detection\n",
      "494.0\n",
      "307200\n",
      "0.0016080729166666667\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "494.0\n",
      "307200\n",
      "0.0016080729166666667\n",
      "STEP 1: Edge Detection\n",
      "494.0\n",
      "307200\n",
      "0.0016080729166666667\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "502.0\n",
      "307200\n",
      "0.0016341145833333333\n",
      "STEP 1: Edge Detection\n",
      "494.0\n",
      "307200\n",
      "0.0016080729166666667\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "28.0\n",
      "307200\n",
      "9.114583333333334e-05\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "0.0\n",
      "307200\n",
      "0.0\n",
      "STEP 1: Edge Detection\n",
      "10.0\n",
      "307200\n",
      "3.255208333333333e-05\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "0.0\n",
      "307200\n",
      "0.0\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "0.0\n",
      "307200\n",
      "0.0\n",
      "STEP 1: Edge Detection\n",
      "0.0\n",
      "307200\n",
      "0.0\n",
      "STEP 1: Edge Detection\n",
      "0.0\n",
      "307200\n",
      "0.0\n",
      "STEP 1: Edge Detection\n",
      "0.0\n",
      "307200\n",
      "0.0\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "42772.0\n",
      "307200\n",
      "0.13923177083333332\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "44226.0\n",
      "307200\n",
      "0.14396484375\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "45552.0\n",
      "307200\n",
      "0.14828125\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "64.0\n",
      "307200\n",
      "0.00020833333333333335\n",
      "STEP 1: Edge Detection\n",
      "76.0\n",
      "307200\n",
      "0.00024739583333333335\n",
      "STEP 1: Edge Detection\n",
      "1.5\n",
      "307200\n",
      "4.8828125e-06\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "72109.0\n",
      "307200\n",
      "0.23472981770833334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "72415.0\n",
      "307200\n",
      "0.23572591145833333\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "71893.0\n",
      "307200\n",
      "0.23402669270833334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "70998.5\n",
      "307200\n",
      "0.23111490885416666\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "69578.0\n",
      "307200\n",
      "0.22649088541666668\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "68648.5\n",
      "307200\n",
      "0.22346516927083335\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "259.0\n",
      "307200\n",
      "0.0008430989583333333\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "59161.0\n",
      "307200\n",
      "0.19258138020833335\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "58405.5\n",
      "307200\n",
      "0.1901220703125\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57951.0\n",
      "307200\n",
      "0.188642578125\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "58192.0\n",
      "307200\n",
      "0.18942708333333333\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "58501.5\n",
      "307200\n",
      "0.1904345703125\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "58858.5\n",
      "307200\n",
      "0.1915966796875\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "59169.5\n",
      "307200\n",
      "0.19260904947916666\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "59491.0\n",
      "307200\n",
      "0.19365559895833334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "59680.0\n",
      "307200\n",
      "0.19427083333333334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "59639.0\n",
      "307200\n",
      "0.19413736979166665\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "59686.5\n",
      "307200\n",
      "0.1942919921875\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "59447.5\n",
      "307200\n",
      "0.19351399739583333\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "59123.0\n",
      "307200\n",
      "0.19245768229166665\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "58945.5\n",
      "307200\n",
      "0.1918798828125\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "58872.0\n",
      "307200\n",
      "0.191640625\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "58601.5\n",
      "307200\n",
      "0.19076009114583334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "58601.5\n",
      "307200\n",
      "0.19076009114583334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "58699.5\n",
      "307200\n",
      "0.1910791015625\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "58839.0\n",
      "307200\n",
      "0.191533203125\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "59060.0\n",
      "307200\n",
      "0.19225260416666667\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "59405.5\n",
      "307200\n",
      "0.19337727864583334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "59753.0\n",
      "307200\n",
      "0.19450846354166668\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "59871.0\n",
      "307200\n",
      "0.194892578125\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "60371.5\n",
      "307200\n",
      "0.19652180989583334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "60585.0\n",
      "307200\n",
      "0.197216796875\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "60946.0\n",
      "307200\n",
      "0.19839192708333334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "61412.0\n",
      "307200\n",
      "0.19990885416666668\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "61691.0\n",
      "307200\n",
      "0.20081705729166666\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "62774.5\n",
      "307200\n",
      "0.20434407552083333\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "63142.0\n",
      "307200\n",
      "0.20554036458333333\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "63477.0\n",
      "307200\n",
      "0.206630859375\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "63880.0\n",
      "307200\n",
      "0.20794270833333334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "64137.0\n",
      "307200\n",
      "0.208779296875\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "64216.0\n",
      "307200\n",
      "0.20903645833333334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "64355.0\n",
      "307200\n",
      "0.20948893229166668\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "64527.5\n",
      "307200\n",
      "0.21005045572916667\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "64664.0\n",
      "307200\n",
      "0.21049479166666665\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "64664.0\n",
      "307200\n",
      "0.21049479166666665\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "64887.0\n",
      "307200\n",
      "0.211220703125\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "64659.5\n",
      "307200\n",
      "0.21048014322916667\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "64771.5\n",
      "307200\n",
      "0.2108447265625\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "64657.0\n",
      "307200\n",
      "0.21047200520833334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "64518.5\n",
      "307200\n",
      "0.21002115885416667\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "64659.0\n",
      "307200\n",
      "0.210478515625\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "64773.0\n",
      "307200\n",
      "0.210849609375\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "65003.0\n",
      "307200\n",
      "0.21159830729166668\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "65514.0\n",
      "307200\n",
      "0.21326171875\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "65773.5\n",
      "307200\n",
      "0.2141064453125\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "66030.5\n",
      "307200\n",
      "0.21494303385416666\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "66319.5\n",
      "307200\n",
      "0.2158837890625\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "66148.5\n",
      "307200\n",
      "0.2153271484375\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "66148.5\n",
      "307200\n",
      "0.2153271484375\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "66262.5\n",
      "307200\n",
      "0.2156982421875\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "66148.5\n",
      "307200\n",
      "0.2153271484375\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "66263.0\n",
      "307200\n",
      "0.21569986979166667\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "66267.5\n",
      "307200\n",
      "0.21571451822916668\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "66639.0\n",
      "307200\n",
      "0.216923828125\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "66750.0\n",
      "307200\n",
      "0.21728515625\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "66780.0\n",
      "307200\n",
      "0.2173828125\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "66927.0\n",
      "307200\n",
      "0.217861328125\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "67218.0\n",
      "307200\n",
      "0.21880859375\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "67432.5\n",
      "307200\n",
      "0.2195068359375\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "67910.5\n",
      "307200\n",
      "0.22106282552083334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "68147.0\n",
      "307200\n",
      "0.22183268229166667\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "68032.0\n",
      "307200\n",
      "0.22145833333333334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "67710.0\n",
      "307200\n",
      "0.22041015625\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "67041.5\n",
      "307200\n",
      "0.21823404947916666\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "66084.0\n",
      "307200\n",
      "0.2151171875\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "65155.0\n",
      "307200\n",
      "0.21209309895833334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "64127.5\n",
      "307200\n",
      "0.20874837239583333\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "63280.0\n",
      "307200\n",
      "0.20598958333333334\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "62433.0\n",
      "307200\n",
      "0.203232421875\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "61819.0\n",
      "307200\n",
      "0.20123372395833333\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "61185.0\n",
      "307200\n",
      "0.199169921875\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "60659.0\n",
      "307200\n",
      "0.19745768229166666\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "60385.5\n",
      "307200\n",
      "0.1965673828125\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "60137.0\n",
      "307200\n",
      "0.19575846354166668\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "60277.0\n",
      "307200\n",
      "0.19621419270833335\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "STEP 1: Edge Detection\n",
      "59811.5\n",
      "307200\n",
      "0.19469889322916667\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "59593.5\n",
      "307200\n",
      "0.1939892578125\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "59700.0\n",
      "307200\n",
      "0.1943359375\n",
      "STEP 2: Find contours of paper\n",
      "STEP 1: Edge Detection\n",
      "59839.5\n",
      "307200\n",
      "0.1947900390625\n",
      "STEP 2: Find contours of paper\n"
     ]
    }
   ],
   "source": [
    "# 명함인식 구현하기 - 웹캠(1)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def auto_scan_image_via_webcam():\n",
    "    \n",
    "    try: \n",
    "        # 카메라를 불러옴\n",
    "        cap = cv2.VideoCapture(0)\n",
    "    except:\n",
    "        print ('cannot load camera!')\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print ('cannot load camera!')\n",
    "            break\n",
    "            \n",
    "        k = cv2.waitKey(10)\n",
    "        if k == 27:\n",
    "            break\n",
    "        \n",
    "        # convert the image to grayscale, blur it, and find edges\n",
    "        # in the image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "        # show the original image and the edge detected image\n",
    "        print (\"STEP 1: Edge Detection\")\n",
    "\n",
    "        # find the contours in the edged image, keeping only the\n",
    "        # largest ones, and initialize the screen contour\n",
    "        (_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # approximate the contour\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "            screenCnt = []\n",
    "\n",
    "            # if our approximated contour has four points, then we\n",
    "            # can assume that we have found our screen\n",
    "            if len(approx) == 4:\n",
    "                contourSize = cv2.contourArea(approx)\n",
    "                # 불필요한 사각형을 검출하지 않기 위한 로직\n",
    "                # 카메라 사이즈\n",
    "                camSize = frame.shape[0] * frame.shape[1]\n",
    "                # 외곽이 10퍼가 넘었을 경우 검출\n",
    "                ratio = contourSize / camSize\n",
    "                print (contourSize)\n",
    "                print (camSize)\n",
    "                print (ratio)\n",
    "                \n",
    "                if ratio > 0.1:\n",
    "                    screenCnt = approx\n",
    "                    \n",
    "                break \n",
    "        \n",
    "        if len(screenCnt) == 0:\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # show the contour (outline) of the piece of paper\n",
    "            print (\"STEP 2: Find contours of paper\")\n",
    "\n",
    "            # 웹캠화면위에 외곽을 보여줌\n",
    "            cv2.drawContours(frame, [screenCnt], -1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "        \n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image_via_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Edge Detection\n",
      "80613.0\n",
      "307200\n",
      "0.262412109375\n",
      "STEP 2: Find contours of paper\n",
      "STEP 3: Apply perspective transform\n",
      "STEP 4: Apply Adaptive Threshold\n"
     ]
    }
   ],
   "source": [
    "# 명함인식 구현하기 - 웹캠(2)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def order_points(pts):\n",
    "    # initialzie a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "def auto_scan_image_via_webcam():\n",
    "    \n",
    "    try: \n",
    "        cap = cv2.VideoCapture(0)\n",
    "    except:\n",
    "        print ('cannot load camera!')\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print ('cannot load camera!')\n",
    "            break\n",
    "            \n",
    "        k = cv2.waitKey(10)\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "        # convert the image to grayscale, blur it, and find edges\n",
    "        # in the image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "        # show the original image and the edge detected image\n",
    "        print (\"STEP 1: Edge Detection\")\n",
    "\n",
    "        # find the contours in the edged image, keeping only the\n",
    "        # largest ones, and initialize the screen contour\n",
    "        (_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # approximate the contour\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "            screenCnt = []\n",
    "\n",
    "            # if our approximated contour has four points, then we\n",
    "            # can assume that we have found our screen\n",
    "            if len(approx) == 4:\n",
    "                contourSize = cv2.contourArea(approx)\n",
    "                camSize = frame.shape[0] * frame.shape[1]\n",
    "                ratio = contourSize / camSize\n",
    "                print (contourSize)\n",
    "                print (camSize)\n",
    "                print (ratio)\n",
    "                \n",
    "                if ratio > 0.1:\n",
    "                    screenCnt = approx\n",
    "                    \n",
    "                break \n",
    "        \n",
    "        if len(screenCnt) == 0:\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # show the contour (outline) of the piece of paper\n",
    "            print (\"STEP 2: Find contours of paper\")\n",
    "\n",
    "            cv2.drawContours(frame, [screenCnt], -1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            \n",
    "            # apply the four point transform to obtain a top-down\n",
    "            # view of the original image\n",
    "            rect = order_points(screenCnt.reshape(4, 2))\n",
    "            (topLeft, topRight, bottomRight, bottomLeft) = rect\n",
    "\n",
    "            w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "            w2 = abs(topRight[0] - topLeft[0])\n",
    "            h1 = abs(topRight[1] - bottomRight[1])\n",
    "            h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "            maxWidth = max([w1, w2])\n",
    "            maxHeight = max([h1, h2])\n",
    "\n",
    "            dst = np.float32([[0,0], [maxWidth-1,0], \n",
    "                              [maxWidth-1,maxHeight-1], [0,maxHeight-1]])\n",
    "\n",
    "            M = cv2.getPerspectiveTransform(rect, dst)\n",
    "            warped = cv2.warpPerspective(frame, M, (maxWidth, maxHeight))\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print (\"STEP 3: Apply perspective transform\")\n",
    "\n",
    "            # convert the warped image to grayscale, then threshold it\n",
    "            # to give it that 'black and white' paper effect\n",
    "            warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "            warped = cv2.adaptiveThreshold(warped, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print (\"STEP 4: Apply Adaptive Threshold\")\n",
    "\n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "    cv2.imshow(\"Scanned\", warped)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image_via_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vpstg OAR O\n",
      "Special Coupon\n",
      "\n",
      "“ PARKSEUNGCHOL\n",
      "HAIRSTUDIO\n"
     ]
    }
   ],
   "source": [
    "# OCR - Tesseract\n",
    "\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "def ocr_tesseract():\n",
    "    image_file = 'images/scannedImage.png'\n",
    "    im = Image.open(image_file)\n",
    "    text = pytesseract.image_to_string(im)\n",
    "    im.show()\n",
    "\n",
    "    print (text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ocr_tesseract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 11001] getaddrinfo failed\n"
     ]
    }
   ],
   "source": [
    "# OCR - Project Oxford by MS\n",
    "\n",
    "from PIL import Image\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n",
    "\n",
    "def print_text(json_data):\n",
    "    result = json.loads(json_data)\n",
    "    for l in result['regions']:\n",
    "        for w in l['lines']:\n",
    "            line = []\n",
    "            for r in w['words']:\n",
    "                line.append(r['text'])\n",
    "            print (' '.join(line))\n",
    "    return\n",
    "\n",
    "def ocr_project_oxford(headers, params, data):\n",
    "    conn = http.client.HTTPSConnection('westcentral.api.cognitive.microsoft.com')\n",
    "    conn.request(\"POST\", \"/vision/v1.0/ocr?%s\" % params, data, headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode()\n",
    "    print (data + \"\\n\")\n",
    "    print_text(data)\n",
    "    conn.close()\n",
    "    return\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/octet-stream',\n",
    "        'Ocp-Apim-Subscription-Key': 'b9da2e3eb7094bf5b77d35831d55edb6',\n",
    "    }\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters\n",
    "        'language': 'unk',\n",
    "        'detectOrientation ': 'true',\n",
    "    })\n",
    "    data = open('images/scannedImage.png', 'rb').read()\n",
    "    \n",
    "    try:\n",
    "        image_file = 'images/scannedImage.png'\n",
    "        im = Image.open(image_file)\n",
    "        im.show()\n",
    "        ocr_project_oxford(headers, params, data)\n",
    "    except Exception as e:\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Find contours of paper\n",
      "STEP 3: Apply perspective transform\n",
      "STEP 4: Apply Adaptive Threshold\n",
      "{ \"statusCode\": 401, \"message\": \"Access denied due to invalid subscription key. Make sure to provide a valid key for an active subscription.\" }\n",
      "\n",
      "'regions'\n"
     ]
    }
   ],
   "source": [
    "# 명함인식 구현하기 - 웹캠 + OCR\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n",
    "\n",
    "def print_text(json_data):\n",
    "    result = json.loads(json_data)\n",
    "    for l in result['regions']:\n",
    "        for w in l['lines']:\n",
    "            line = []\n",
    "            for r in w['words']:\n",
    "                line.append(r['text'])\n",
    "            print (' '.join(line))\n",
    "    return\n",
    "\n",
    "def ocr_project_oxford(headers, params, data):\n",
    "    conn = http.client.HTTPSConnection('westus.api.cognitive.microsoft.com')\n",
    "    conn.request(\"POST\", \"/vision/v1.0/ocr?%s\" % params, data, headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode()\n",
    "    print (data + \"\\n\")\n",
    "    print_text(data)\n",
    "    conn.close()\n",
    "    return\n",
    "\n",
    "def order_points(pts):\n",
    "    # initialzie a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "def auto_scan_image_via_webcam():\n",
    "    \n",
    "    try: \n",
    "        cap = cv2.VideoCapture(0)\n",
    "    except:\n",
    "        print ('cannot load camera')\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print ('cannot load camera!')\n",
    "            break\n",
    "            \n",
    "        k = cv2.waitKey(10)\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "        # convert the image to grayscale, blur it, and find edges\n",
    "        # in the image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "        # show the original image and the edge detected image\n",
    "        # print (\"STEP 1: Edge Detection\")\n",
    "\n",
    "        # find the contours in the edged image, keeping only the\n",
    "        # largest ones, and initialize the screen contour\n",
    "        (_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # approximate the contour\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "            screenCnt = []\n",
    "\n",
    "            # if our approximated contour has four points, then we\n",
    "            # can assume that we have found our screen\n",
    "            if len(approx) == 4:\n",
    "                contourSize = cv2.contourArea(approx)\n",
    "                camSize = frame.shape[0] * frame.shape[1]\n",
    "                ratio = contourSize / camSize\n",
    "                # print (contourSize)\n",
    "                # print (camSize)\n",
    "                # print (ratio)\n",
    "                \n",
    "                if ratio > 0.1:\n",
    "                    screenCnt = approx\n",
    "                    \n",
    "                break \n",
    "        \n",
    "        if len(screenCnt) == 0:\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # show the contour (outline) of the piece of paper\n",
    "            print (\"STEP 2: Find contours of paper\")\n",
    "\n",
    "            cv2.drawContours(frame, [screenCnt], -1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            \n",
    "            # apply the four point transform to obtain a top-down\n",
    "            # view of the original image\n",
    "            rect = order_points(screenCnt.reshape(4, 2))\n",
    "            (topLeft, topRight, bottomRight, bottomLeft) = rect\n",
    "\n",
    "            w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "            w2 = abs(topRight[0] - topLeft[0])\n",
    "            h1 = abs(topRight[1] - bottomRight[1])\n",
    "            h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "            maxWidth = max([w1, w2])\n",
    "            maxHeight = max([h1, h2])\n",
    "\n",
    "            dst = np.float32([[0,0], [maxWidth-1,0], \n",
    "                              [maxWidth-1,maxHeight-1], [0,maxHeight-1]])\n",
    "\n",
    "            M = cv2.getPerspectiveTransform(rect, dst)\n",
    "            warped = cv2.warpPerspective(frame, M, (maxWidth, maxHeight))\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print (\"STEP 3: Apply perspective transform\")\n",
    "\n",
    "            # convert the warped image to grayscale, then threshold it\n",
    "            # to give it that 'black and white' paper effect\n",
    "            warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "            warped = cv2.adaptiveThreshold(warped, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print (\"STEP 4: Apply Adaptive Threshold\")\n",
    "\n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "    cv2.imshow(\"Scanned\", warped)\n",
    "    cv2.imwrite('scannedImage.png', warped)\n",
    "    \n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/octet-stream',\n",
    "        'Ocp-Apim-Subscription-Key': '',\n",
    "    }\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters\n",
    "        'language': 'unk',\n",
    "        'detectOrientation ': 'true',\n",
    "    })\n",
    "    data = open('scannedImage.png', 'rb').read()\n",
    "    \n",
    "    try:\n",
    "        image_file = 'scannedImage.png'\n",
    "        ocr_project_oxford(headers, params, data)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image_via_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/byeondongnam/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:23: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/byeondongnam/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:27: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 548.0 -> 811.0 / 1161.0 (0.429037520392), 24339 -> 97293 / 356816 (2.9974115616911132), 0.626602103602 -> 0.712642158862\n",
      "1 811.0 -> 831.0 / 1161.0 (0.0571428571429), 97293 -> 97293 / 356816 (0.0), 0.712642158862 -> 0.721499799725\n",
      "(80, 80, 419, 367) -> (80, 80, 434, 367)\n",
      "(80, 80, 434, 367) -> (44, 80, 434, 367)\n",
      "(44, 80, 434, 367) -> (44, 80, 456, 367)\n",
      "images/scannedImage.png -> croppedImage.png\n"
     ]
    }
   ],
   "source": [
    "# (참고) OpenCV - 이미지에서 텍스트 영역만 찾아내기\n",
    "\n",
    "# 출처: http://www.danvk.org/2015/01/07/finding-blocks-of-text-in-an-image-using-python-opencv-and-numpy.html\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import rank_filter\n",
    "\n",
    "\n",
    "def dilate(ary, N, iterations):\n",
    "    \"\"\"Dilate using an NxN '+' sign shape. ary is np.uint8.\"\"\"\n",
    "    kernel = np.zeros((N,N), dtype=np.uint8)\n",
    "    kernel[(N-1)/2,:] = 1\n",
    "    dilated_image = cv2.dilate(ary / 255, kernel, iterations=iterations)\n",
    "\n",
    "    kernel = np.zeros((N,N), dtype=np.uint8)\n",
    "    kernel[:,(N-1)/2] = 1\n",
    "    dilated_image = cv2.dilate(dilated_image, kernel, iterations=iterations)\n",
    "    dilated_image = cv2.convertScaleAbs(dilated_image)\n",
    "    return dilated_image\n",
    "\n",
    "\n",
    "def props_for_contours(contours, ary):\n",
    "    \"\"\"Calculate bounding box & the number of set pixels for each contour.\"\"\"\n",
    "    c_info = []\n",
    "    for c in contours:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        c_im = np.zeros(ary.shape)\n",
    "        cv2.drawContours(c_im, [c], 0, 255, -1)\n",
    "        c_info.append({\n",
    "            'x1': x,\n",
    "            'y1': y,\n",
    "            'x2': x + w - 1,\n",
    "            'y2': y + h - 1,\n",
    "            'sum': np.sum(ary * (c_im > 0))/255\n",
    "        })\n",
    "    return c_info\n",
    "\n",
    "\n",
    "def union_crops(crop1, crop2):\n",
    "    \"\"\"Union two (x1, y1, x2, y2) rects.\"\"\"\n",
    "    x11, y11, x21, y21 = crop1\n",
    "    x12, y12, x22, y22 = crop2\n",
    "    return min(x11, x12), min(y11, y12), max(x21, x22), max(y21, y22)\n",
    "\n",
    "\n",
    "def intersect_crops(crop1, crop2):\n",
    "    x11, y11, x21, y21 = crop1\n",
    "    x12, y12, x22, y22 = crop2\n",
    "    return max(x11, x12), max(y11, y12), min(x21, x22), min(y21, y22)\n",
    "\n",
    "\n",
    "def crop_area(crop):\n",
    "    x1, y1, x2, y2 = crop\n",
    "    return max(0, x2 - x1) * max(0, y2 - y1)\n",
    "\n",
    "\n",
    "def find_border_components(contours, ary):\n",
    "    borders = []\n",
    "    area = ary.shape[0] * ary.shape[1]\n",
    "    for i, c in enumerate(contours):\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        if w * h > 0.5 * area:\n",
    "            borders.append((i, x, y, x + w - 1, y + h - 1))\n",
    "    return borders\n",
    "\n",
    "\n",
    "def angle_from_right(deg):\n",
    "    return min(deg % 90, 90 - (deg % 90))\n",
    "\n",
    "\n",
    "def remove_border(contour, ary):\n",
    "    \"\"\"Remove everything outside a border contour.\"\"\"\n",
    "    # Use a rotated rectangle (should be a good approximation of a border).\n",
    "    # If it's far from a right angle, it's probably two sides of a border and\n",
    "    # we should use the bounding box instead.\n",
    "    c_im = np.zeros(ary.shape)\n",
    "    r = cv2.minAreaRect(contour)\n",
    "    degs = r[2]\n",
    "    if angle_from_right(degs) <= 10.0:\n",
    "        box = cv2.boxPoints(r)\n",
    "        box = np.int0(box)\n",
    "        cv2.drawContours(c_im, [box], 0, 255, -1)\n",
    "        cv2.drawContours(c_im, [box], 0, 0, 4)\n",
    "    else:\n",
    "        x1, y1, x2, y2 = cv2.boundingRect(contour)\n",
    "        cv2.rectangle(c_im, (x1, y1), (x2, y2), 255, -1)\n",
    "        cv2.rectangle(c_im, (x1, y1), (x2, y2), 0, 4)\n",
    "\n",
    "    return np.minimum(c_im, ary)\n",
    "\n",
    "\n",
    "def find_components(edges, max_components=16):\n",
    "    \"\"\"Dilate the image until there are just a few connected components.\n",
    "    Returns contours for these components.\"\"\"\n",
    "    # Perform increasingly aggressive dilation until there are just a few\n",
    "    # connected components.\n",
    "    count = 21\n",
    "    dilation = 5\n",
    "    n = 1\n",
    "    while count > 16:\n",
    "        n += 1\n",
    "        dilated_image = dilate(edges, N=3, iterations=n)\n",
    "        _, contours, hierarchy = cv2.findContours(dilated_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        count = len(contours)\n",
    "    #print dilation\n",
    "    #Image.fromarray(edges).show()\n",
    "    #Image.fromarray(255 * dilated_image).show()\n",
    "    return contours\n",
    "\n",
    "\n",
    "def find_optimal_components_subset(contours, edges):\n",
    "    \"\"\"Find a crop which strikes a good balance of coverage/compactness.\n",
    "    Returns an (x1, y1, x2, y2) tuple.\n",
    "    \"\"\"\n",
    "    c_info = props_for_contours(contours, edges)\n",
    "    c_info.sort(key=lambda x: -x['sum'])\n",
    "    total = np.sum(edges) / 255\n",
    "    area = edges.shape[0] * edges.shape[1]\n",
    "\n",
    "    c = c_info[0]\n",
    "    del c_info[0]\n",
    "    this_crop = c['x1'], c['y1'], c['x2'], c['y2']\n",
    "    crop = this_crop\n",
    "    covered_sum = c['sum']\n",
    "\n",
    "    while covered_sum < total:\n",
    "        changed = False\n",
    "        recall = 1.0 * covered_sum / total\n",
    "        prec = 1 - 1.0 * crop_area(crop) / area\n",
    "        f1 = 2 * (prec * recall / (prec + recall))\n",
    "        #print '----'\n",
    "        for i, c in enumerate(c_info):\n",
    "            this_crop = c['x1'], c['y1'], c['x2'], c['y2']\n",
    "            new_crop = union_crops(crop, this_crop)\n",
    "            new_sum = covered_sum + c['sum']\n",
    "            new_recall = 1.0 * new_sum / total\n",
    "            new_prec = 1 - 1.0 * crop_area(new_crop) / area\n",
    "            new_f1 = 2 * new_prec * new_recall / (new_prec + new_recall)\n",
    "\n",
    "            # Add this crop if it improves f1 score,\n",
    "            # _or_ it adds 25% of the remaining pixels for <15% crop expansion.\n",
    "            # ^^^ very ad-hoc! make this smoother\n",
    "            remaining_frac = c['sum'] / (total - covered_sum)\n",
    "            new_area_frac = 1.0 * crop_area(new_crop) / crop_area(crop) - 1\n",
    "            if new_f1 > f1 or (\n",
    "                    remaining_frac > 0.25 and new_area_frac < 0.15):\n",
    "                print('%d %s -> %s / %s (%s), %s -> %s / %s (%s), %s -> %s' % (\n",
    "                        i, covered_sum, new_sum, total, remaining_frac,\n",
    "                        crop_area(crop), crop_area(new_crop), area, new_area_frac,\n",
    "                        f1, new_f1))\n",
    "                crop = new_crop\n",
    "                covered_sum = new_sum\n",
    "                del c_info[i]\n",
    "                changed = True\n",
    "                break\n",
    "\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    return crop\n",
    "\n",
    "\n",
    "def pad_crop(crop, contours, edges, border_contour, pad_px=15):\n",
    "    \"\"\"Slightly expand the crop to get full contours.\n",
    "    This will expand to include any contours it currently intersects, but will\n",
    "    not expand past a border.\n",
    "    \"\"\"\n",
    "    bx1, by1, bx2, by2 = 0, 0, edges.shape[0], edges.shape[1]\n",
    "    if border_contour is not None and len(border_contour) > 0:\n",
    "        c = props_for_contours([border_contour], edges)[0]\n",
    "        bx1, by1, bx2, by2 = c['x1'] + 5, c['y1'] + 5, c['x2'] - 5, c['y2'] - 5\n",
    "\n",
    "    def crop_in_border(crop):\n",
    "        x1, y1, x2, y2 = crop\n",
    "        x1 = max(x1 - pad_px, bx1)\n",
    "        y1 = max(y1 - pad_px, by1)\n",
    "        x2 = min(x2 + pad_px, bx2)\n",
    "        y2 = min(y2 + pad_px, by2)\n",
    "        return crop\n",
    "\n",
    "    crop = crop_in_border(crop)\n",
    "\n",
    "    c_info = props_for_contours(contours, edges)\n",
    "    changed = False\n",
    "    for c in c_info:\n",
    "        this_crop = c['x1'], c['y1'], c['x2'], c['y2']\n",
    "        this_area = crop_area(this_crop)\n",
    "        int_area = crop_area(intersect_crops(crop, this_crop))\n",
    "        new_crop = crop_in_border(union_crops(crop, this_crop))\n",
    "        if 0 < int_area < this_area and crop != new_crop:\n",
    "            print('%s -> %s' % (str(crop), str(new_crop)))\n",
    "            changed = True\n",
    "            crop = new_crop\n",
    "\n",
    "    if changed:\n",
    "        return pad_crop(crop, contours, edges, border_contour, pad_px)\n",
    "    else:\n",
    "        return crop\n",
    "\n",
    "\n",
    "def downscale_image(im, max_dim=2048):\n",
    "    \"\"\"Shrink im until its longest dimension is <= max_dim.\n",
    "    Returns new_image, scale (where scale <= 1).\n",
    "    \"\"\"\n",
    "    a = im.shape[0]\n",
    "    b = im.shape[1]\n",
    "    if max(a, b) <= max_dim:\n",
    "        return 1.0, im\n",
    "\n",
    "    scale = 1.0 * max_dim / max(a, b)\n",
    "    dim = (int(a * scale), int(b * scale))\n",
    "    new_im = cv2.resize(im, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    return scale, new_im\n",
    "\n",
    "\n",
    "def process_image(path, out_path):\n",
    "    orig_im = Image.open(path)\n",
    "    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    scale, im = downscale_image(im)\n",
    "\n",
    "    edges = cv2.Canny(im, 100, 200)\n",
    "\n",
    "    # TODO: dilate image _before_ finding a border. This is crazy sensitive!\n",
    "    _, contours, hierarchy = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    borders = find_border_components(contours, edges)\n",
    "    borders.sort(key=lambda i_x1_y1_x2_y2: (i_x1_y1_x2_y2[3] - i_x1_y1_x2_y2[1]) * (i_x1_y1_x2_y2[4] - i_x1_y1_x2_y2[2]))\n",
    "\n",
    "    border_contour = None\n",
    "    if len(borders):\n",
    "        border_contour = contours[borders[0][0]]\n",
    "        edges = remove_border(border_contour, edges)\n",
    "\n",
    "    edges = 255 * (edges > 0).astype(np.uint8)\n",
    "\n",
    "    # Remove ~1px borders using a rank filter.\n",
    "    maxed_rows = rank_filter(edges, -5, size=(1, 20))\n",
    "    maxed_cols = rank_filter(edges, -5, size=(20, 1))\n",
    "    debordered = np.minimum(np.minimum(edges, maxed_rows), maxed_cols)\n",
    "    edges = debordered\n",
    "\n",
    "    contours = find_components(edges)\n",
    "    if len(contours) == 0:\n",
    "        print('%s -> (no text!)' % path)\n",
    "        return\n",
    "\n",
    "    crop = find_optimal_components_subset(contours, edges)\n",
    "    crop = pad_crop(crop, contours, edges, border_contour)\n",
    "\n",
    "    crop = [int(x / scale) for x in crop]  # upscale to the original image size.\n",
    "\n",
    "    # draw and show cropped rectangle area in the original image\n",
    "    rgb_im = orig_im.convert('RGB')\n",
    "    draw = ImageDraw.Draw(rgb_im)\n",
    "    draw.rectangle(crop, outline='red')\n",
    "    rgb_im.show()\n",
    "\n",
    "    text_im = orig_im.crop(crop)\n",
    "    text_im.show()\n",
    "    text_im.save(out_path)\n",
    "    print('%s -> %s' % (path, out_path))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # path = 'images/text.jpg'\n",
    "    path = 'images/scannedImage.png'\n",
    "    out_path = 'croppedImage.png'\n",
    "    try:\n",
    "        process_image(path, out_path)\n",
    "    except Exception as e:\n",
    "        print('%s %s' % (path, e))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
